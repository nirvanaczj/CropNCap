{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMvMKp2RDqZQ",
        "outputId": "1b7d6a61-0a80-4d64-d62a-ac4eee9b7874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zHThOHkBbVmr4tfBk4cf_d8ib-9VzRDT\n",
            "To: /content/checkpoint_captioning_83000.zip\n",
            "270MB [00:04, 60.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Library , Dataset, and Model\n",
        "#Download Pre-trained Checkpoints on Captioning\n",
        "\n",
        "!gdown --id 1zHThOHkBbVmr4tfBk4cf_d8ib-9VzRDT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FC7Omq7F8p4",
        "outputId": "f47f13d0-8629-4196-8abd-09f983b2ebf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  checkpoint_captioning_83000.zip\n",
            "   creating: checkpoints/\n",
            "   creating: checkpoints/train/\n",
            "  inflating: checkpoints/train/ckpt-2.data-00000-of-00001  \n",
            "  inflating: checkpoints/train/checkpoint  \n",
            "  inflating: checkpoints/train/ckpt-1.data-00000-of-00001  \n",
            "  inflating: checkpoints/train/ckpt-1.index  \n",
            "  inflating: checkpoints/train/ckpt-3.index  \n",
            "  inflating: checkpoints/train/ckpt-3.data-00000-of-00001  \n",
            "  inflating: checkpoints/train/ckpt-4.index  \n",
            "  inflating: checkpoints/train/ckpt-2.index  \n",
            "  inflating: checkpoints/train/ckpt-4.data-00000-of-00001  \n"
          ]
        }
      ],
      "source": [
        "# Unzip the Checkpoints\n",
        "\n",
        "!unzip checkpoint_captioning_83000.zip\n",
        "\n",
        "# Here, I used MS-COCO dataset for captioning, The dataset contains over 82,000 images, each of which has at least 5 different caption annotations. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOLB0dA4Bn86",
        "outputId": "58005a49-cc1e-4072-947c-81cc6b2efbb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageOps\n",
        "import time\n",
        "import collections\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "\n",
        "top_k = 5000\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = top_k + 1\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a59vuJjSYckC"
      },
      "source": [
        "Pick an object detection module.\n",
        "* **FasterRCNN+InceptionResNet V2**: high accuracy,\n",
        "* **ssd+mobilenet V2**: small and fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo0PArB9YXnx"
      },
      "outputs": [],
      "source": [
        "module_handle = \"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\" #@param [\"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\", \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"]\n",
        "\n",
        "detector = hub.load(module_handle).signatures['default']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re3peHH75Fpn"
      },
      "outputs": [],
      "source": [
        "## Object Detection\n",
        "\n",
        "def display_image(image):\n",
        "  fig = plt.figure(figsize=(20, 15))\n",
        "  plt.grid(False)\n",
        "  plt.imshow(image)\n",
        "\n",
        "def download_and_resize_image(url, new_width=256, new_height=256,\n",
        "                              display=False):\n",
        "  _, filename = tempfile.mkstemp(suffix=\".jpg\")\n",
        "  response = urlopen(url)\n",
        "  image_data = response.read()\n",
        "  image_data = BytesIO(image_data)\n",
        "  pil_image = Image.open(image_data)\n",
        "  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n",
        "  pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "  pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n",
        "  print(\"Image downloaded to %s.\" % filename)\n",
        "  if display:\n",
        "    display_image(pil_image)\n",
        "  return filename\n",
        "\n",
        "\n",
        "def draw_bounding_box_on_image(image,ymin, xmin,\n",
        "                               ymax, xmax, color,\n",
        "                               font, thickness=4,\n",
        "                               display_str_list=()):\n",
        "\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  im_width, im_height = image.size\n",
        "  (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                ymin * im_height, ymax * im_height)\n",
        "  draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
        "             (left, top)],\n",
        "            width=thickness,\n",
        "            fill=color)\n",
        "\n",
        "  # If the total height of the display strings added to the top of the bounding\n",
        "  # box exceeds the top of the image, stack the strings below the bounding box\n",
        "  # instead of above.\n",
        "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
        "  # Each display_str has a top and bottom margin of 0.05x.\n",
        "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
        "\n",
        "  if top > total_display_str_height:\n",
        "    text_bottom = top\n",
        "  else:\n",
        "    text_bottom = top + total_display_str_height\n",
        "  # Reverse list and print from bottom to top.\n",
        "  for display_str in display_str_list[::-1]:\n",
        "    text_width, text_height = font.getsize(display_str)\n",
        "    margin = np.ceil(0.05 * text_height)\n",
        "    draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
        "                    (left + text_width, text_bottom)],\n",
        "                   fill=color)\n",
        "    draw.text((left + margin, text_bottom - text_height - margin),\n",
        "              display_str,\n",
        "              fill=\"black\",\n",
        "              font=font)\n",
        "    text_bottom -= text_height - 2 * margin\n",
        "\n",
        "\n",
        "# Draw Boxes on the image\n",
        "def draw_boxes(image, boxes, class_names, scores, max_boxes=20, min_score=0.1):\n",
        "  colors = list(ImageColor.colormap.values())\n",
        "\n",
        "  try:\n",
        "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\",\n",
        "                              25)\n",
        "  except IOError:\n",
        "    print(\"Font not found, using default font.\")\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "  for i in range(min(boxes.shape[0], max_boxes)):\n",
        "    if scores[i] >= min_score:\n",
        "      ymin, xmin, ymax, xmax = tuple(boxes[i])\n",
        "      display_str = \"{}: {}%\".format(class_names[i].decode(\"ascii\"),\n",
        "                                     int(100 * scores[i]))\n",
        "      color = colors[hash(class_names[i]) % len(colors)]\n",
        "      image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
        "      draw_bounding_box_on_image(\n",
        "          image_pil,\n",
        "          ymin,\n",
        "          xmin,\n",
        "          ymax,\n",
        "          xmax,\n",
        "          color,\n",
        "          font,\n",
        "          display_str_list=[display_str])\n",
        "      np.copyto(image, np.array(image_pil))\n",
        "  return image\n",
        "\n",
        "def load_img(path):\n",
        "  img = tf.io.read_file(path)\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  return img\n",
        "import requests\n",
        "\n",
        "# Run Detector and Caption on the image.\n",
        "\n",
        "def run_detector(detector, path):\n",
        "  img = load_img(path)\n",
        "\n",
        "  converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
        "  start_time = time.time()\n",
        "  result = detector(converted_img)\n",
        "  end_time = time.time()\n",
        "\n",
        "  result = {key:value.numpy() for key,value in result.items()}\n",
        "\n",
        "  print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n",
        "  print(\"Inference time: \", end_time-start_time)\n",
        "  boxes = result[\"detection_boxes\"][:10]\n",
        "  names = result[\"detection_class_entities\"][:10]\n",
        "  score = result[\"detection_scores\"][:10]\n",
        "  human = []\n",
        "  objects = []\n",
        "  for i in range(len(boxes)):\n",
        "    name = names[i].decode(\"ascii\")\n",
        "    print(boxes[i], score[i])\n",
        "    if name == \"Man\" or name == \"Woman\":\n",
        "      human.append([boxes[i], name, score[i]])\n",
        "    else:\n",
        "      objects.append([boxes[i], name, score[i]])\n",
        "  soloObjects = []\n",
        "  for j in objects:\n",
        "    for k in human:\n",
        "      if check_collision(j[0], k[0]):\n",
        "        k[0][0] = min(j[0][0],k[0][0])\n",
        "        k[0][1] = min(j[0][1],k[0][1])\n",
        "        k[0][2] = max(j[0][2],k[0][2])\n",
        "        k[0][3] = max(j[0][3],k[0][3])\n",
        "        j.append(\"eliminated\")\n",
        "  for h in objects:\n",
        "    if len(h) <= 3:\n",
        "      soloObjects.append(h)\n",
        "  print(\"Found \",len(human), \" human and \", len(soloObjects),\" solo objects.\")\n",
        "  image_with_boxes = draw_boxes(\n",
        "      img.numpy(), result[\"detection_boxes\"],\n",
        "      result[\"detection_class_entities\"], result[\"detection_scores\"])\n",
        "\n",
        "  display_image(image_with_boxes)\n",
        "  endJson = {}\n",
        "  endJson[\"Boxes\"] = []\n",
        "  endJson[\"Caption\"] = []\n",
        "  for h in human:\n",
        "    endJson[\"Boxes\"].append([str(a) for a in list(h[0])])\n",
        "    cap = captioningImage(path, h[0])\n",
        "    endJson[\"Caption\"].append(cap)\n",
        "  for j in soloObjects:\n",
        "    endJson[\"Boxes\"].append([str(a) for a in list(j[0])])\n",
        "    cap = captioningImage(path, j[0])\n",
        "    endJson[\"Caption\"].append(cap)\n",
        "  print(endJson)\n",
        "  with open(path + '.json', 'w') as f:\n",
        "      json.dump(endJson, f)\n",
        "    \n",
        "# Function to Caption Image.\n",
        "\n",
        "def captioningImage(url, box):\n",
        "\n",
        "  pil_image = Image.open(url)\n",
        "  width, height = pil_image.size\n",
        "\n",
        "  ymin, xmin, ymax, xmax = tuple(box)\n",
        "  # Setting the points for cropped image\n",
        "  left = xmin * width\n",
        "  top = ymin * height\n",
        "  right = xmax * width\n",
        "  bottom = ymax * height\n",
        "  \n",
        "  # Cropped image of above dimension\n",
        "  # (It will not change original image)\n",
        "\n",
        "  pil_image = pil_image.crop((left, top, right, bottom))\n",
        "  pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "  pil_image_rgb.save(\"tmpimage\", format=\"JPEG\", quality=90)\n",
        "  display_image(pil_image_rgb)\n",
        "  result, attention_plot = evaluate(\"tmpimage\")\n",
        "  print('Prediction Caption:', ' '.join(result))\n",
        "  return ' '.join(result)\n",
        "\n",
        "\n",
        "# Function to Check Collision.\n",
        "\n",
        "def check_collision (tobject, thuman):\n",
        "  ymi,xmi,yma,xma = tuple(tobject)\n",
        "  ymiH,xmiH,ymaH,xmaH = tuple(thuman)\n",
        "  #If any of the sides from object are outside of human\n",
        "  if xmi > xmaH: return False\n",
        "  if xma < xmiH: return False\n",
        "  if ymi > ymaH: return False\n",
        "  if yma < ymiH: return False\n",
        "  #If none of the sides from object is outside human\n",
        "  return True;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33-fHapXGR0C"
      },
      "outputs": [],
      "source": [
        "# Captioining Attention and Encoders for captioning.\n",
        "# SHOW AND TELL MODEL\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
        "                                         self.W2(hidden_with_time_axis)))\n",
        "    score = self.V(attention_hidden_layer)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x\n",
        "\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    output, state = self.gru(x)\n",
        "    x = self.fc1(output)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "    x = self.fc2(x)\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))\n",
        "\n",
        "embedding_dim = 256\n",
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4CS1QVCGHOC",
        "outputId": "6231e6bb-d6c3-4a9c-e709-157fcba82ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "252878848/252872794 [==============================] - 17s 0us/step\n",
            "252887040/252872794 [==============================] - 17s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "87924736/87910968 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Evaluate Code on Captioning\n",
        "\n",
        "\n",
        "# restoring the latest checkpoint in checkpoint_path\n",
        "checkpoint_path = \"/content/checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer=optimizer)\n",
        "ckpt.restore(tf.train.latest_checkpoint(checkpoint_path))\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "\n",
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "\n",
        "\n",
        "def evaluate(image):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
        "                                                 -1,\n",
        "                                                 img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input,\n",
        "                                                         features,\n",
        "                                                         hidden)\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot\n",
        "\n",
        "annotation_folder = '/annotations/'\n",
        "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
        "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
        "                                           extract=True)\n",
        "  os.remove(annotation_zip)\n",
        "\n",
        "annotation_file = '/content/annotations/captions_train2014.json'\n",
        "with open(annotation_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "train_captions = []\n",
        "\n",
        "for val in annotations['annotations']:\n",
        "  caption = f\"<start> {val['caption']} <end>\"\n",
        "  train_captions.append(caption)\n",
        "\n",
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
        "\n",
        "top_k = 5000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~')\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path\n",
        "\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'\n",
        "\n",
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "max_length = calc_max_length(train_seqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTnA40xEN5bg",
        "outputId": "7b30a383-1c77-453b-e7f4-6d0fbeefaf71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-09-29 02:28:41--  https://variety.com/wp-content/uploads/2019/09/modern-love.jpg?w=1000\n",
            "Resolving variety.com (variety.com)... 192.0.66.176\n",
            "Connecting to variety.com (variety.com)|192.0.66.176|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 362574 (354K) [image/jpeg]\n",
            "Saving to: ‘test.jpg’\n",
            "\n",
            "test.jpg            100%[===================>] 354.08K  1.22MB/s    in 0.3s    \n",
            "\n",
            "2021-09-29 02:28:42 (1.22 MB/s) - ‘test.jpg’ saved [362574/362574]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download one Image\n",
        "!wget --output-document=test.jpg https://variety.com/wp-content/uploads/2019/09/modern-love.jpg?w=1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z5SQFGwr67rU"
      },
      "outputs": [],
      "source": [
        "# Run the Detector on the Image, a corresponded Json will be saved\n",
        "run_detector(detector, \"test.jpg\")  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}